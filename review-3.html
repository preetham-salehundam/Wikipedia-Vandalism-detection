<!doctype html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport"
          content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <title>Review on Vandalism detection in Wikipedia using WikiTrust</title>
</head>
<style>
    .container {
        display: flex;
        flex-direction: column;
        width:90%;
    }
    .justify{
        text-align:justify;
    }
    .no-bullets {
        list-style: none;
    }
</style>
<body>
<!-- UIkit CSS -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/css/uikit.min.css" />

<!-- UIkit JS -->
<script src="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/js/uikit.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/uikit@3.5.3/dist/js/uikit-icons.min.js"></script>
<div class="uk-container">
    <article class="uk-article uk-text-justify">
        <h1 class="uk-article-title uk-text-center">Review on Detecting Wikipedia Vandalism using WikiTrust</h1>
        <p class="uk-article-meta uk-text-center">written by <a href="https://preetham-salehundam.github.io">Preetham Salehundam</a></p>
        <p class="uk-text-center"> Keywords: <span class="uk-badge">Content Vandalism</span><span class="uk-badge">Wikipedia Vandalism</span><span class="uk-badge">Wiki Vandalism</span><span class="uk-badge">spam filtering</span><span class="uk-badge">Anamoly detection</span><span class="uk-badge">Machine Learning</span><span class="uk-badge">Text mining</span><span class="uk-badge">Reputation based filtering systems</span></p>
        <p>Wikipedia Vandalism detection using a trust-based system called WikiTrust. This solution aligns with our area of interest, i.e. Vandalism detection.  Wikipedia itself uses the features computed by wiki trust. Machine Learning algorithms have been used on Wikipedia edit metadata along with author and content reputation to detect Vandalism. WikiTrust works well with both online and historical vandalism detection.</p>
        <p>The features generated by WikiTrust capture the latent patterns of vandalism using the edit metadata alone. However, it ignores the influence of author and patterns in his editing preferences. In our approach, the relation between an edit and an article is modelled as a bi-partite graph problem. Using graph deep learning methods, node embeddings can be generated which captures both structural and hand-crafted features. These embeddings therefore could be used for better identification of Vandalism</p>
        <p>This work mainly is concerned about how to extract the features from edit metadata. Given the revision content and current edit metadata, how author and content reputation scores can be computed. It uses a decision tree algorithm, C4.5 to compute the probability of vandalism of each revision.</p>
        <p>This study uses PAN Wikipedia 2010 workshop dataset to train and evaluate the tool. In our approach, we use the same dataset to train and evaluate our graph-based solution.</p>
        <table class="uk-table uk-table-small uk-table-middle uk-table-divider">
            <thead>
            <tr>
                <th>Method</th>
                <th>Precision</th>
                <th>Recall</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <th>WikiTrust</th>
                <td>48.5%</td>
                <td>83.5%</td>
            </tr>
            <tr>
                <th>Our Approach</th>
                <td>61.0%</td>
                <td>64.0%</td>
            </tr>
            </tbody>
        </table>
        <p>Our Approach does better in avoiding any false positives. Since, deep learning techniques need huge data to model the edit and article interactions. Our model underperforms when compared to WikiTrust which is based on static features.</p>
        <p>The current state of the art techniques includes the use of bots with regex and rules. These bots have 100% precision but very poor recall. Hence, the revisions identified as vandalistic revision are further verified by human reviewers. The issue of subjective reviews is still prevalent when human reviewers are involved. Hence, we propose to model this interaction as a graph problem and address the problem.</p>
    </article>
</div>
</body>
</html>